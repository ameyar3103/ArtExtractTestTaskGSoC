{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mameyar3103\u001b[0m (\u001b[33mameyar3103-iiit-hyderabad\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ameya/Pictures/ArtExtractTestTaskGSoC/wandb/run-20250318_100402-uig19va2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet/runs/uig19va2' target=\"_blank\">stilted-jazz-1</a></strong> to <a href='https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet' target=\"_blank\">https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet/runs/uig19va2' target=\"_blank\">https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet/runs/uig19va2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet/runs/uig19va2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x784a39c45660>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(entity=\"ameyar3103-iiit-hyderabad\",project=\"recurrent_conv_art_effnet\", config={\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 4,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"model\": \"EFFNET_RCNN\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('wikiart_csv/artist_train.csv',header=None, names=[\"image_path\", \"artist_id\"])\n",
    "df_val = pd.read_csv('wikiart_csv/artist_val.csv',header=None, names=[\"image_path\", \"artist_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of classes\n",
    "num_classes = 23 # from artist_class.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather input data\n",
    "train_images = df_train['image_path'].values\n",
    "train_labels = df_train['artist_id'].values\n",
    "\n",
    "val_images = df_val['image_path'].values\n",
    "val_labels = df_val['artist_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data and create test and train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Post_Impressionism/vincent-van-gogh_pollard-willows-and-setting-sun-1888.jpg', 'Art_Nouveau_Modern/boris-kustodiev_village-holiday-1910.jpg', 'Romanticism/gustave-dore_he-sprang-unpon-the-old-woman-and-ate-her-up.jpg', 'Northern_Renaissance/albrecht-durer_three-studies-from-nature-for-adam-s-arms-1504.jpg', 'Symbolism/nicholas-roerich_untitled-1915.jpg', 'Symbolism/nicholas-roerich_himalayas-22.jpg', 'Impressionism/eugene-boudin_cows-near-the-toques.jpg', 'Realism/john-singer-sargent_sheepfold-in-the-tirol-1915.jpg', 'Expressionism/pablo-picasso_the-absinthe-drinker-portrait-of-angel-fernandez-de-soto-1903.jpg', 'Symbolism/nicholas-roerich_kangchenjunga-2.jpg', 'Realism/vincent-van-gogh_beach-and-sea-1882(1).jpg', 'Symbolism/nicholas-roerich_ladakh-golden-clouds-over-blue-mountains-1943.jpg', 'Romanticism/gustave-dore_abraham-god-and-two-angels-png-1852.jpg', 'Impressionism/camille-pissarro_windmill-at-knokke-belgium-1894.jpg', 'Realism/vincent-van-gogh_the-spire-of-the-church-of-our-lady-1885.jpg', 'Impressionism/camille-pissarro_portrait-of-madame-felicie-vellay-estruc.jpg', 'Realism/ivan-shishkin_forest-path-1863.jpg', 'Realism/ivan-shishkin_landscape-1896.jpg', 'Post_Impressionism/vincent-van-gogh_two-trees.jpg', 'Post_Impressionism/vincent-van-gogh_skull.jpg', 'Symbolism/martiros-saryan_irises-1903.jpg', 'Impressionism/camille-pissarro_the-louvre-morning-sun-1901.jpg', 'Post_Impressionism/vincent-van-gogh_orchard-with-blossoming-apricot-trees-1888.jpg', 'Expressionism/martiros-saryan_yerevan-1923.jpg', 'Realism/vincent-van-gogh_fisherman-in-jacket-with-upturned-collar-1883(1).jpg', 'Impressionism/camille-pissarro_the-thaw-eragny-1893.jpg', 'Art_Nouveau_Modern/nicholas-roerich_prologue-forest-1908.jpg', 'Impressionism/claude-monet_relaxing-in-the-garden-argenteuil.jpg', 'Realism/martiros-saryan_portrait-of-a-i-alikhanov.jpg', 'Impressionism/pierre-auguste-renoir_laundry-boat-by-the-banks-of-the-seine-near-paris-1873.jpg', 'Impressionism/pierre-auguste-renoir_studies-of-the-children-of-paul-berard-1881.jpg', 'Impressionism/pierre-auguste-renoir_reading-couple-edmond-renoir-and-marguerite-legrand-1877.jpg')\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# create test and train dataset for dataloader\n",
    "\n",
    "def get_image(image_path,image_size=224):\n",
    "    try:\n",
    "        img = cv2.imread('./wikiart/' + image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Image not loaded: ./wikiart/{image_path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = img.shape\n",
    "        scale = 256 / min(h, w)\n",
    "        new_w = int(w * scale)\n",
    "        new_h = int(h * scale)\n",
    "        img_resized = cv2.resize(img, (new_w, new_h))\n",
    "        start_x = (new_w - image_size) // 2\n",
    "        start_y = (new_h - image_size) // 2\n",
    "        img_cropped = img_resized[start_y:start_y+image_size, start_x:start_x+image_size]\n",
    "        img_cropped = img_cropped.astype(np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_cropped).permute(2, 0, 1)\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        img_tensor = (img_tensor - mean) / std\n",
    "        return img_tensor\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return torch.zeros(3, image_size, image_size)\n",
    "\n",
    "class WikiArtDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image_vectors = []\n",
    "        # for image in self.images:\n",
    "        #     image_emb = get_image(image)\n",
    "        #     image_vectors.append(image_emb)\n",
    "        # image = torch.stack(image_vectors)\n",
    "        image = self.images[idx]\n",
    "        # label should be a one-hot encoded vector\n",
    "        label = torch.zeros(num_classes)\n",
    "        label[self.labels[idx]] = 1\n",
    "\n",
    "        return image, label\n",
    "\n",
    "train_dataset = WikiArtDataset(train_images, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = WikiArtDataset(val_images, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print(images)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "class EffNetLSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # EfficientNet-B0 backbone (outputs 1280 channels)\n",
    "        effnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        self.cnn = effnet.features  \n",
    "        \n",
    "        self.channel_reducer = nn.Sequential(\n",
    "            nn.Conv2d(1280, 512, kernel_size=1),  \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        \n",
    "        x = self.channel_reducer(features)  \n",
    "        bs, c, h, w = x.size()\n",
    "        x = x.permute(0, 2, 3, 1).reshape(bs, h*w, c)  \n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        last_hidden = torch.cat((h_n[-2], h_n[-1]), dim=1)  \n",
    "        \n",
    "        return self.classifier(last_hidden)\n",
    "    \n",
    "model = EffNetLSTM(num_classes)\n",
    "model.to('cuda')\n",
    "\n",
    "# Loss and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.cnn.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.channel_reducer.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.lstm.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  29%|██▉       | 121/418 [01:17<02:47,  1.78it/s, loss=2.85]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 1/20:  59%|█████▉    | 247/418 [02:36<01:40,  1.70it/s, loss=2.15]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 1/20: 100%|██████████| 418/418 [04:23<00:00,  1.59it/s, loss=2.46]\n",
      "Validation: 100%|██████████| 179/179 [01:27<00:00,  2.05it/s, loss=2.17] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 2.4257, Val Loss: 1.7476, Val Accuracy: 49.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:  31%|███       | 130/418 [01:18<02:46,  1.73it/s, loss=1.48]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 2/20:  46%|████▌     | 193/418 [01:56<02:19,  1.61it/s, loss=1.69]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 2/20: 100%|██████████| 418/418 [04:12<00:00,  1.66it/s, loss=3.52] \n",
      "Validation: 100%|██████████| 179/179 [01:21<00:00,  2.20it/s, loss=2.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train Loss: 1.6529, Val Loss: 1.4455, Val Accuracy: 58.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:  38%|███▊      | 158/418 [01:38<02:46,  1.57it/s, loss=1.3]  Corrupt JPEG data: bad Huffman code\n",
      "Epoch 3/20:  84%|████████▍ | 352/418 [03:42<00:37,  1.76it/s, loss=0.982]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 3/20: 100%|██████████| 418/418 [04:21<00:00,  1.60it/s, loss=1.7]  \n",
      "Validation: 100%|██████████| 179/179 [01:23<00:00,  2.14it/s, loss=1.85] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train Loss: 1.3759, Val Loss: 1.2750, Val Accuracy: 63.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:   8%|▊         | 34/418 [00:21<03:51,  1.66it/s, loss=1.3]  Corrupt JPEG data: premature end of data segment\n",
      "Epoch 4/20:  68%|██████▊   | 283/418 [02:58<01:26,  1.57it/s, loss=1.31] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 4/20: 100%|██████████| 418/418 [04:22<00:00,  1.59it/s, loss=4.01] \n",
      "Validation: 100%|██████████| 179/179 [01:23<00:00,  2.14it/s, loss=1.76] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train Loss: 1.1793, Val Loss: 1.1903, Val Accuracy: 65.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:  26%|██▌       | 109/418 [01:06<03:29,  1.47it/s, loss=0.775]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 5/20:  38%|███▊      | 158/418 [01:36<02:52,  1.51it/s, loss=1.04] Corrupt JPEG data: premature end of data segment\n",
      "Epoch 5/20: 100%|██████████| 418/418 [04:18<00:00,  1.62it/s, loss=2.71] \n",
      "Validation: 100%|██████████| 179/179 [01:24<00:00,  2.12it/s, loss=1.79] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train Loss: 1.0197, Val Loss: 1.1001, Val Accuracy: 68.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:  64%|██████▍   | 269/418 [02:46<01:28,  1.69it/s, loss=0.958]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 6/20:  79%|███████▉  | 332/418 [03:25<00:49,  1.74it/s, loss=0.853]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 6/20: 100%|██████████| 418/418 [04:17<00:00,  1.62it/s, loss=2.03] \n",
      "Validation: 100%|██████████| 179/179 [01:24<00:00,  2.13it/s, loss=1.55] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train Loss: 0.9011, Val Loss: 1.0193, Val Accuracy: 71.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:  63%|██████▎   | 263/418 [02:41<01:46,  1.46it/s, loss=0.368]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 7/20:  82%|████████▏ | 341/418 [03:30<00:47,  1.62it/s, loss=0.648]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 7/20: 100%|██████████| 418/418 [04:17<00:00,  1.62it/s, loss=2.85] \n",
      "Validation: 100%|██████████| 179/179 [01:25<00:00,  2.10it/s, loss=1.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train Loss: 0.7918, Val Loss: 1.0017, Val Accuracy: 71.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:  51%|█████▏    | 215/418 [02:12<02:01,  1.67it/s, loss=0.845]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 8/20:  91%|█████████ | 380/418 [03:55<00:21,  1.75it/s, loss=0.504]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 8/20: 100%|██████████| 418/418 [04:19<00:00,  1.61it/s, loss=4.05] \n",
      "Validation: 100%|██████████| 179/179 [01:23<00:00,  2.15it/s, loss=1.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train Loss: 0.7042, Val Loss: 0.9478, Val Accuracy: 73.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20:  24%|██▍       | 100/418 [00:59<02:57,  1.79it/s, loss=0.236]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 9/20:  45%|████▍     | 187/418 [01:51<02:08,  1.79it/s, loss=0.509]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 9/20: 100%|██████████| 418/418 [04:05<00:00,  1.70it/s, loss=5.12] \n",
      "Validation: 100%|██████████| 179/179 [01:18<00:00,  2.28it/s, loss=1.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train Loss: 0.6483, Val Loss: 0.9496, Val Accuracy: 73.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20:   4%|▍         | 16/418 [00:08<03:54,  1.71it/s, loss=0.684]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 10/20:  12%|█▏        | 50/418 [00:28<03:33,  1.72it/s, loss=0.374]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 10/20: 100%|██████████| 418/418 [04:08<00:00,  1.68it/s, loss=1.88] \n",
      "Validation: 100%|██████████| 179/179 [01:18<00:00,  2.28it/s, loss=2.12]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train Loss: 0.5608, Val Loss: 0.9432, Val Accuracy: 74.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20:  42%|████▏     | 174/418 [01:40<02:17,  1.78it/s, loss=0.461]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 11/20:  53%|█████▎    | 220/418 [02:08<01:58,  1.68it/s, loss=0.468]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 11/20: 100%|██████████| 418/418 [04:02<00:00,  1.72it/s, loss=3.39] \n",
      "Validation: 100%|██████████| 179/179 [01:17<00:00,  2.30it/s, loss=0.906] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train Loss: 0.5230, Val Loss: 0.9450, Val Accuracy: 74.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20:  49%|████▉     | 204/418 [01:58<01:58,  1.81it/s, loss=0.493]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 12/20:  64%|██████▍   | 267/418 [02:35<01:25,  1.77it/s, loss=0.5]  Corrupt JPEG data: premature end of data segment\n",
      "Epoch 12/20: 100%|██████████| 418/418 [04:03<00:00,  1.72it/s, loss=2.96] \n",
      "Validation: 100%|██████████| 179/179 [01:18<00:00,  2.28it/s, loss=1.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train Loss: 0.4724, Val Loss: 0.9030, Val Accuracy: 75.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20:  64%|██████▍   | 269/418 [02:34<01:27,  1.69it/s, loss=0.31]  Corrupt JPEG data: bad Huffman code\n",
      "Epoch 13/20:  74%|███████▍  | 309/418 [02:58<01:05,  1.66it/s, loss=0.0834]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 13/20: 100%|██████████| 418/418 [04:02<00:00,  1.72it/s, loss=2.82]  \n",
      "Validation: 100%|██████████| 179/179 [01:17<00:00,  2.31it/s, loss=1.12] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train Loss: 0.4297, Val Loss: 0.9521, Val Accuracy: 75.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20:  53%|█████▎    | 222/418 [02:08<01:52,  1.74it/s, loss=0.378] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 14/20:  77%|███████▋  | 322/418 [03:06<00:58,  1.63it/s, loss=0.523] Corrupt JPEG data: premature end of data segment\n",
      "Epoch 14/20: 100%|██████████| 418/418 [04:01<00:00,  1.73it/s, loss=2.25]  \n",
      "Validation: 100%|██████████| 179/179 [01:18<00:00,  2.27it/s, loss=0.955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train Loss: 0.3955, Val Loss: 0.9157, Val Accuracy: 75.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20:  13%|█▎        | 55/418 [00:31<03:31,  1.72it/s, loss=0.259]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 15/20:  28%|██▊       | 117/418 [01:08<02:52,  1.75it/s, loss=0.12] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 15/20: 100%|██████████| 418/418 [04:01<00:00,  1.73it/s, loss=3.1]   \n",
      "Validation: 100%|██████████| 179/179 [01:17<00:00,  2.30it/s, loss=1.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Train Loss: 0.3445, Val Loss: 0.9240, Val Accuracy: 76.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20:  32%|███▏      | 135/418 [01:18<02:35,  1.82it/s, loss=0.501]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 16/20:  89%|████████▉ | 372/418 [03:35<00:26,  1.71it/s, loss=0.199] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 16/20: 100%|██████████| 418/418 [04:01<00:00,  1.73it/s, loss=2.94]  \n",
      "Validation: 100%|██████████| 179/179 [01:17<00:00,  2.31it/s, loss=1.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 - Train Loss: 0.3159, Val Loss: 0.9512, Val Accuracy: 76.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:  17%|█▋        | 73/418 [00:41<03:10,  1.81it/s, loss=0.419]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 17/20:  41%|████      | 170/418 [01:38<02:18,  1.80it/s, loss=0.242] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 17/20: 100%|██████████| 418/418 [04:01<00:00,  1.73it/s, loss=0.0301]\n",
      "Validation: 100%|██████████| 179/179 [01:18<00:00,  2.29it/s, loss=1.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 - Train Loss: 0.3042, Val Loss: 0.9682, Val Accuracy: 77.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:  21%|██        | 87/418 [00:50<03:18,  1.67it/s, loss=0.547] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 18/20:  67%|██████▋   | 280/418 [02:43<01:19,  1.74it/s, loss=0.197] Corrupt JPEG data: premature end of data segment\n",
      "Epoch 18/20: 100%|██████████| 418/418 [04:01<00:00,  1.73it/s, loss=0.507] \n",
      "Validation: 100%|██████████| 179/179 [01:18<00:00,  2.29it/s, loss=0.746] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 - Train Loss: 0.2655, Val Loss: 0.9756, Val Accuracy: 76.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20:  70%|██████▉   | 292/418 [02:51<01:09,  1.82it/s, loss=0.69]  Corrupt JPEG data: premature end of data segment\n",
      "Epoch 19/20:  90%|█████████ | 377/418 [03:39<00:24,  1.69it/s, loss=0.445] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 19/20: 100%|██████████| 418/418 [04:01<00:00,  1.73it/s, loss=3.1]   \n",
      "Validation: 100%|██████████| 179/179 [01:17<00:00,  2.32it/s, loss=1.28]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 - Train Loss: 0.2548, Val Loss: 0.9450, Val Accuracy: 76.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:   8%|▊         | 32/418 [00:19<03:55,  1.64it/s, loss=0.512]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 20/20:  64%|██████▍   | 267/418 [02:34<01:26,  1.74it/s, loss=0.0613]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 20/20: 100%|██████████| 418/418 [04:01<00:00,  1.73it/s, loss=2]     \n",
      "Validation: 100%|██████████| 179/179 [01:17<00:00,  2.31it/s, loss=0.977]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 - Train Loss: 0.2577, Val Loss: 0.9774, Val Accuracy: 76.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for image_paths, labels in train_bar:\n",
    "        image_tensors = torch.stack([get_image(image_path) for image_path in image_paths])\n",
    "        images = image_tensors.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    wandb.log({\"epoch\": epoch+1, \"train_loss\": avg_train_loss})\n",
    "    \n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader, desc=\"Validation\")\n",
    "        for image_paths, labels in val_bar:\n",
    "            image_tensors = torch.stack([get_image(image_path) for image_path in image_paths])\n",
    "            image_tensors = image_tensors.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "            outputs = model(image_tensors)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
    "            val_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy})\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    if(epoch%10==0):\n",
    "        torch.save(model.state_dict(), f\"effnet_rcnn_epoch_{epoch+1}.pth\")\n",
    "        torch.save(optimizer.state_dict(), f\"effnet_rcnn_optimizer_epoch_{epoch+1}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
