{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mameyar3103\u001b[0m (\u001b[33mameyar3103-iiit-hyderabad\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ameya/Pictures/ArtExtractTestTaskGSoC/wandb/run-20250323_154444-88de65cq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet/runs/88de65cq' target=\"_blank\">volcanic-haze-6</a></strong> to <a href='https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet' target=\"_blank\">https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet/runs/88de65cq' target=\"_blank\">https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet/runs/88de65cq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ameyar3103-iiit-hyderabad/recurrent_conv_art_effnet/runs/88de65cq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x714a24c95720>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(entity=\"ameyar3103-iiit-hyderabad\",project=\"recurrent_conv_art_effnet\", config={\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"model\": \"EFFNET_RCNN\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('wikiart_csv/style_train.csv',header=None, names=[\"image_path\", \"style_id\"])\n",
    "df_val = pd.read_csv('wikiart_csv/style_val.csv',header=None, names=[\"image_path\", \"style_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of classes\n",
    "num_classes = 27 # from style_class.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather input data\n",
    "train_images = df_train['image_path'].values\n",
    "train_labels = df_train['style_id'].values\n",
    "\n",
    "val_images = df_val['image_path'].values\n",
    "val_labels = df_val['style_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data and create test and train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Post_Impressionism/paul-gauguin_and-the-gold-of-their-bodies-1901.jpg', 'Rococo/maurice-quentin-de-la-tour_portrait-of-madame-de-pompadour-1752.jpg', 'Fauvism/aldemir-martins_galo-1987.jpg', 'Baroque/adriaen-van-ostade_portrait-of-a-boy.jpg', 'Baroque/jan-steen_merry-company-on-a-terrace-1675.jpg', 'Baroque/francisco-de-zurbaran_st-romanus-and-st-barulas-of-antioch-1638.jpg', 'Post_Impressionism/pierre-bonnard_girl-with-a-dog-in-the-park-at-grand-lemps-also-known-as-dauphine-1900.jpg', 'Realism/gustave-courbet_the-white-sail-1877.jpg', 'Impressionism/walter-sickert_mrs-barrett.jpg', 'Rococo/fyodor-rokotov_portrait-of-catherine-ii-repeat-version-of-a-portrait-after-1768.jpg', 'Realism/konstantin-somov_lady-in-blue-portrait-of-the-artist-yelizaveta-martynova-1900.jpg', 'Expressionism/martiros-saryan_in-armenia-1923.jpg', 'Realism/giovanni-boldini_portrait-of-guiseppe-verdi-1813-1901-1886.jpg', 'Impressionism/pierre-auguste-renoir_still-life-with-pomegranates.jpg', 'High_Renaissance/luca-signorelli_dante-and-virgil-entering-purgatory-1502.jpg', 'Pop_Art/claes-oldenburg_free-stamp-at-cleveland-city-hall-collaboration-with-van-bruggen.jpg')\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# create test and train dataset for dataloader\n",
    "\n",
    "def get_image(image_path,image_size=224):\n",
    "    try:\n",
    "        img = cv2.imread('./wikiart/' + image_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Image not loaded: ./wikiart/{image_path}\")\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = img.shape\n",
    "        scale = 256 / min(h, w)\n",
    "        new_w = int(w * scale)\n",
    "        new_h = int(h * scale)\n",
    "        img_resized = cv2.resize(img, (new_w, new_h))\n",
    "        start_x = (new_w - image_size) // 2\n",
    "        start_y = (new_h - image_size) // 2\n",
    "        img_cropped = img_resized[start_y:start_y+image_size, start_x:start_x+image_size]\n",
    "        img_cropped = img_cropped.astype(np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_cropped).permute(2, 0, 1)\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        std  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "        img_tensor = (img_tensor - mean) / std\n",
    "        return img_tensor\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return torch.zeros(3, image_size, image_size)\n",
    "\n",
    "class WikiArtDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image_vectors = []\n",
    "        # for image in self.images:\n",
    "        #     image_emb = get_image(image)\n",
    "        #     image_vectors.append(image_emb)\n",
    "        # image = torch.stack(image_vectors)\n",
    "        image = self.images[idx]\n",
    "        # label should be a one-hot encoded vector\n",
    "        label = torch.zeros(num_classes)\n",
    "        label[self.labels[idx]] = 1\n",
    "\n",
    "        return image, label\n",
    "\n",
    "train_dataset = WikiArtDataset(train_images, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataset = WikiArtDataset(val_images, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print(images)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class EffNetLSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # EfficientNet-B0 backbone (outputs 1280 channels)\n",
    "        effnet = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        self.cnn = effnet.features  \n",
    "        \n",
    "        self.channel_reducer = nn.Sequential(\n",
    "            nn.Conv2d(1280, 512, kernel_size=1),  \n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)  \n",
    "        \n",
    "        x = self.channel_reducer(features)  \n",
    "        bs, c, h, w = x.size()\n",
    "        x = x.permute(0, 2, 3, 1).reshape(bs, h*w, c)  \n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        last_hidden = torch.cat((h_n[-2], h_n[-1]), dim=1)  \n",
    "        \n",
    "        return self.classifier(last_hidden)\n",
    "    \n",
    "model = EffNetLSTM(num_classes)\n",
    "model.to('cuda')\n",
    "\n",
    "# Loss and optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.cnn.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.channel_reducer.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.lstm.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:  48%|████▊     | 1718/3565 [10:12<11:26,  2.69it/s, loss=2.23]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 1/20:  83%|████████▎ | 2958/3565 [17:25<03:57,  2.55it/s, loss=2.17]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 1/20: 100%|██████████| 3565/3565 [20:57<00:00,  2.83it/s, loss=3.05]\n",
      "Validation: 100%|██████████| 1527/1527 [06:48<00:00,  3.74it/s, loss=2.99] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train Loss: 2.1877, Val Loss: 1.8370, Val Accuracy: 40.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:   5%|▌         | 180/3565 [01:02<17:15,  3.27it/s, loss=1.86]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 2/20:  81%|████████▏ | 2905/3565 [16:46<04:00,  2.74it/s, loss=1.45] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 2/20: 100%|██████████| 3565/3565 [20:40<00:00,  2.87it/s, loss=1.93] \n",
      "Validation: 100%|██████████| 1527/1527 [06:40<00:00,  3.81it/s, loss=2.75] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Train Loss: 1.8250, Val Loss: 1.6779, Val Accuracy: 45.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:  95%|█████████▍| 3372/3565 [19:11<00:57,  3.34it/s, loss=1.84] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 3/20:  98%|█████████▊| 3503/3565 [20:00<00:21,  2.92it/s, loss=1.39] Corrupt JPEG data: premature end of data segment\n",
      "Epoch 3/20: 100%|██████████| 3565/3565 [20:23<00:00,  2.91it/s, loss=2.12] \n",
      "Validation: 100%|██████████| 1527/1527 [06:45<00:00,  3.76it/s, loss=2.6]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Train Loss: 1.6678, Val Loss: 1.5599, Val Accuracy: 48.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:  60%|█████▉    | 2137/3565 [12:19<06:57,  3.42it/s, loss=1.59] Corrupt JPEG data: premature end of data segment\n",
      "Epoch 4/20:  81%|████████  | 2894/3565 [17:01<03:16,  3.42it/s, loss=1.69] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 4/20: 100%|██████████| 3565/3565 [21:09<00:00,  2.81it/s, loss=2.71] \n",
      "Validation: 100%|██████████| 1527/1527 [06:55<00:00,  3.67it/s, loss=2.29] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Train Loss: 1.5504, Val Loss: 1.5222, Val Accuracy: 50.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:  43%|████▎     | 1529/3565 [08:54<10:25,  3.25it/s, loss=1.84] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 5/20:  60%|██████    | 2141/3565 [12:28<09:31,  2.49it/s, loss=1.32] Corrupt JPEG data: premature end of data segment\n",
      "Epoch 5/20: 100%|██████████| 3565/3565 [20:49<00:00,  2.85it/s, loss=1.22] \n",
      "Validation: 100%|██████████| 1527/1527 [06:25<00:00,  3.96it/s, loss=2.3]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Train Loss: 1.4688, Val Loss: 1.4633, Val Accuracy: 51.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:  31%|███       | 1093/3565 [06:07<14:14,  2.89it/s, loss=1.49] Corrupt JPEG data: premature end of data segment\n",
      "Epoch 6/20:  68%|██████▊   | 2409/3565 [13:36<05:56,  3.24it/s, loss=2.02] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 6/20: 100%|██████████| 3565/3565 [20:15<00:00,  2.93it/s, loss=3.74] \n",
      "Validation: 100%|██████████| 1527/1527 [06:26<00:00,  3.95it/s, loss=2]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Train Loss: 1.3947, Val Loss: 1.4699, Val Accuracy: 51.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:  63%|██████▎   | 2233/3565 [12:26<08:28,  2.62it/s, loss=1.05] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 7/20:  90%|████████▉ | 3194/3565 [18:23<02:38,  2.34it/s, loss=1.02] Corrupt JPEG data: premature end of data segment\n",
      "Epoch 7/20: 100%|██████████| 3565/3565 [20:41<00:00,  2.87it/s, loss=6.95] \n",
      "Validation: 100%|██████████| 1527/1527 [06:55<00:00,  3.68it/s, loss=2.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Train Loss: 1.3154, Val Loss: 1.4130, Val Accuracy: 54.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:  62%|██████▏   | 2206/3565 [12:48<06:47,  3.33it/s, loss=1.07] Corrupt JPEG data: premature end of data segment\n",
      "Epoch 8/20:  90%|████████▉ | 3198/3565 [18:46<02:03,  2.97it/s, loss=1.39] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 8/20: 100%|██████████| 3565/3565 [20:56<00:00,  2.84it/s, loss=0.741]\n",
      "Validation: 100%|██████████| 1527/1527 [06:59<00:00,  3.64it/s, loss=1.74] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Train Loss: 1.2510, Val Loss: 1.3922, Val Accuracy: 54.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20:  14%|█▍        | 491/3565 [03:02<17:49,  2.88it/s, loss=0.577]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 9/20:  49%|████▉     | 1754/3565 [10:39<11:43,  2.57it/s, loss=1.27] Corrupt JPEG data: premature end of data segment\n",
      "Epoch 9/20: 100%|██████████| 3565/3565 [21:50<00:00,  2.72it/s, loss=4.02] \n",
      "Validation: 100%|██████████| 1527/1527 [07:20<00:00,  3.47it/s, loss=1.58] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Train Loss: 1.1916, Val Loss: 1.3341, Val Accuracy: 56.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20:   6%|▋         | 224/3565 [01:19<21:12,  2.62it/s, loss=0.931]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 10/20:  50%|█████     | 1800/3565 [11:07<09:32,  3.08it/s, loss=0.867]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 10/20: 100%|██████████| 3565/3565 [21:35<00:00,  2.75it/s, loss=1.08] \n",
      "Validation: 100%|██████████| 1527/1527 [06:51<00:00,  3.71it/s, loss=2.02] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Train Loss: 1.1321, Val Loss: 1.3832, Val Accuracy: 55.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20:   3%|▎         | 96/3565 [00:34<19:56,  2.90it/s, loss=1.32] Corrupt JPEG data: premature end of data segment\n",
      "Epoch 11/20:  58%|█████▊    | 2061/3565 [12:14<11:01,  2.27it/s, loss=1.68] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 11/20: 100%|██████████| 3565/3565 [21:02<00:00,  2.82it/s, loss=2.94] \n",
      "Validation: 100%|██████████| 1527/1527 [06:34<00:00,  3.87it/s, loss=1.73] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 - Train Loss: 1.0710, Val Loss: 1.3593, Val Accuracy: 56.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20:   7%|▋         | 254/3565 [01:24<15:14,  3.62it/s, loss=0.753]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 12/20:  43%|████▎     | 1532/3565 [08:46<11:35,  2.92it/s, loss=1]    Corrupt JPEG data: premature end of data segment\n",
      "Epoch 12/20: 100%|██████████| 3565/3565 [20:30<00:00,  2.90it/s, loss=1.01] \n",
      "Validation: 100%|██████████| 1527/1527 [06:36<00:00,  3.85it/s, loss=1.86] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 - Train Loss: 1.0099, Val Loss: 1.3726, Val Accuracy: 56.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20:  29%|██▉       | 1048/3565 [05:54<15:43,  2.67it/s, loss=0.536]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 13/20:  86%|████████▌ | 3059/3565 [17:26<03:39,  2.30it/s, loss=0.653]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 13/20: 100%|██████████| 3565/3565 [20:26<00:00,  2.91it/s, loss=2.25] \n",
      "Validation: 100%|██████████| 1527/1527 [06:36<00:00,  3.85it/s, loss=1.95] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 - Train Loss: 0.9635, Val Loss: 1.4224, Val Accuracy: 56.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20:  67%|██████▋   | 2381/3565 [13:40<06:36,  2.98it/s, loss=1.57] Corrupt JPEG data: bad Huffman code\n",
      "Epoch 14/20:  69%|██████▉   | 2452/3565 [14:05<05:38,  3.29it/s, loss=1.16] Corrupt JPEG data: premature end of data segment\n",
      "Epoch 14/20: 100%|██████████| 3565/3565 [20:53<00:00,  2.84it/s, loss=4.8]  \n",
      "Validation: 100%|██████████| 1527/1527 [07:13<00:00,  3.52it/s, loss=2.21] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 - Train Loss: 0.9141, Val Loss: 1.3958, Val Accuracy: 56.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20:  46%|████▌     | 1632/3565 [09:59<11:23,  2.83it/s, loss=0.893]Corrupt JPEG data: bad Huffman code\n",
      "Epoch 15/20:  97%|█████████▋| 3462/3565 [21:02<00:39,  2.60it/s, loss=0.699]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 15/20: 100%|██████████| 3565/3565 [21:40<00:00,  2.74it/s, loss=8.51] \n",
      "Validation: 100%|██████████| 1527/1527 [06:51<00:00,  3.71it/s, loss=2.95] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 - Train Loss: 0.8691, Val Loss: 1.4523, Val Accuracy: 55.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20:  26%|██▌       | 926/3565 [05:32<15:09,  2.90it/s, loss=0.872]Corrupt JPEG data: premature end of data segment\n",
      "Epoch 16/20:  65%|██████▍   | 2309/3565 [13:51<07:32,  2.78it/s, loss=0.99] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m train_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_paths, labels \u001b[38;5;129;01min\u001b[39;00m train_bar:\n\u001b[0;32m----> 9\u001b[0m     image_tensors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([get_image(image_path) \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m image_paths])\n\u001b[1;32m     10\u001b[0m     images \u001b[38;5;241m=\u001b[39m image_tensors\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m train_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_paths, labels \u001b[38;5;129;01min\u001b[39;00m train_bar:\n\u001b[0;32m----> 9\u001b[0m     image_tensors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mget_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m image_paths])\n\u001b[1;32m     10\u001b[0m     images \u001b[38;5;241m=\u001b[39m image_tensors\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mget_image\u001b[0;34m(image_path, image_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_image\u001b[39m(image_path,image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./wikiart/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage not loaded: ./wikiart/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for image_paths, labels in train_bar:\n",
    "        image_tensors = torch.stack([get_image(image_path) for image_path in image_paths])\n",
    "        images = image_tensors.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    wandb.log({\"epoch\": epoch+1, \"train_loss\": avg_train_loss})\n",
    "    \n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader, desc=\"Validation\")\n",
    "        for image_paths, labels in val_bar:\n",
    "            image_tensors = torch.stack([get_image(image_path) for image_path in image_paths])\n",
    "            image_tensors = image_tensors.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "            outputs = model(image_tensors)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
    "            val_bar.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    wandb.log({\"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy})\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "    if(epoch%5==0):\n",
    "        torch.save(model.state_dict(), f\"effnet_rcnn_epoch_{epoch+1}_style.pth\")\n",
    "        torch.save(optimizer.state_dict(), f\"effnet_rcnn_optimizer_epoch_{epoch+1}_style.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
